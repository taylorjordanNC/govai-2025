:imagesdir: ../assets/images
[#deploy-intro]
= Module 1: Deploying your model with vLLM on OpenShift AI

== Introduction

This module covers deploying a Gen AI model with vLLM as the inference runtime on an OpenShift AI cluster.

== Learning Objectives

- Deploy model with vLLM as the inference runtime
- Understand platform-specific configuration requirements
- Deploy Llama Stack operator on OpenShift AI 

== Prerequisites

- Access to target deployment environments (RHOAI OpenShift cluster with 1 L4 GPU node)
- OpenShift Familiarity
- Basic understanding of Agentic concepts

== Preparing our environment

You currently are working with an OpenShift cluster with OpenShift AI deployed on top of that kubernetes cluster. We have one GPU node. 

NOTE: You will need to complete this lab as Admin user.

=== Clone the workshop repository

To complete the workshop successfully, clone the workshop repository:

[source,console,role="execute"]
----
git clone https://github.com/taylorjordanNC/lls-workshop.git
cd lls-workshop
----
