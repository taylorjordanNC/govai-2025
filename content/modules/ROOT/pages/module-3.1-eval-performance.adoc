:imagesdir: ../assets/images

[#eval-performance]
= Evaluating Model Performance with GuideLLM
// TODO: Review and add more info around the parameters being changed in the different runs.
In Generative AI systems, evaluating system performance including latency, throughput, and resource utilization is just as important as evaluating model accuracy or quality. Here's why:

. **User Experience**: High latency leads to sluggish interactions, which is unacceptable in chatbots, copilots, and real-time applications. Users expect sub-second responses.

. **Scalability**: Throughput determines how many requests a system can handle in parallel. For enterprise GenAI apps, high throughput is essential to serve multiple users or integrate with high-frequency backend processes.

. **Cost Efficiency**: Slow or inefficient systems require more compute to serve the same volume of requests. Optimizing performance directly reduces cloud GPU costs and improves ROI.

. **Fair Benchmarking**: A model may appear “better” in isolated evaluation, but if it requires excessive inference time or hardware, it may not be viable in production. True model evaluation must balance quality and performance.

. **Deployment Readiness**: Latency and throughput impact architectural decisions (e.g., batching, caching, distributed serving). Measuring them ensures a model is viable under real-world constraints.

== What is GuideLLM?

GuideLLM is a toolkit for evaluating and optimizing the deployment of LLMs. By simulating real-world inference workloads, GuideLLM enables you to easily assess the performance, resource requirements, and cost implications of deploying LLMs on various hardware configurations. This approach ensures efficient, scalable, and cost-effective LLM inference serving while maintaining high service quality.

GuideLLM is now officially a part of the vLLM upstream project. This toolset is one of the primary ways Red Hat internal teams are benchmarking customer models. GuideLLM will be the main framework we will recommend to our customers with the scope of model performance and optimization. 

== Set Up GuideLLM in a Workbench

There are several current ways you may deploy and use GuideLLM.

* CLI tool: documented in the upstream project.
* Python library: Not yet in current upstream documentation. You can see an example here in this guide: https://redhatquickcourses.github.io/genai-vllm/genai-vllm/1/rhoai_deploy/guide_llm.html. 
* Kubernetes job: https://github.com/rh-aiservices-bu/guidellm-pipeline 
* Tekton pipeline: https://github.com/jhurlocker/guidellm-pipeline.git (forked from above repo) 

For our lab today, we will utilize the CLI tool within a jupyterlab workbench in the OpenShift AI UI. For reproducibility and scalability, we recommend the pipeline approach!

Let's set up our workbench.

=== Navigate to the OpenShift AI UI

* Go to the OpenShift AI dashboard and select the **Data Science Project** view.
* Choose the **lls-demo** project (or create it if it doesn't exist).
* Click on the **Workbenches** tab.

=== Create a Workbench

==== Create a new workbench with these settings:

* **Name**: `guidellm-ws` (or your preferred name)
* **Image**: Select `Standard Data Science`
* **Container size**: Small

==== Wait for the workbench to start (this can take a moment).

==== Once ready, click on the hyperlink to launch the JupyterLab interface.

=== Install GuideLLM

* In the JupyterLab interface, open a **Terminal**.

* Install GuideLLM and required dependencies:

[source,console,role=execute,subs=attributes+]
----
pip install guidellm
----

You're now ready to run GuideLLM benchmarks directly from your workbench.

Before running the pipeline, let's review the options for GuideLLM more closely.

== GuideLLM Arguments

* Peruse the available GuideLLM https://github.com/neuralmagic/guidellm?tab=readme-ov-file#configurations[configuration options]. 
* The GitHub ReadMe gives detailed information about configuration flags

=== Input/Output tokens
For different use cases, you can set different standardized dataset profiles that can be passed in as arguments in GuideLLM. For example, the following variables represent input and output tokens, respectively, based on the given use case: 

* **Chat (512/256)**
* **RAG (4096/512)**
* **Summarization (1024/256)**
* **Code Generation (512/512)**

Using these profiles, we can map specific i/o token scenarios to real-world use cases to make these runs more explainable to how this impacts applications.

=== --rate-type

**--rate-type** defines the type of benchmark to run. By default GuideLLM will do a sweep of available benchmarks, but you may choose to isolate specific benchmark tests. 

* synchronous: Runs a single stream of requests one at a time. --rate must not be set for this mode.
* throughput: Runs all requests in parallel to measure the maximum throughput for the server (bounded by GUIDELLM__MAX_CONCURRENCY config argument). --rate must not be set for this mode.
* concurrent: Runs a fixed number of streams of requests in parallel. --rate must be set to the desired concurrency level/number of streams.
* constant: Sends requests asynchronously at a constant rate set by --rate.
* poisson: Sends requests at a rate following a Poisson distribution with the mean set by --rate.
* sweep: Automatically determines the minimum and maximum rates the server can support by running synchronous and throughput benchmarks, and then runs a series of benchmarks equally spaced between the two rates. The number of benchmarks is set by --rate (default is 10).

=== --data

GuideLLM has a default dataset it uses if you do not specify anything specific. However, the dataset you use should align directly with your use case. 

== Use-Case Specific Data Requirements

=== Training vs Production Data

**This training uses emulated data** for consistency:
```json
{"type":"emulated","prompt_tokens":512,"output_tokens":128}
```
**For client engagements**, use representative data for accurate performance evaluation.

=== Why Using Real Use-Case Specific Data Matters

Real workloads differ significantly from stock data:

- **Token distribution**: Customer support (50-200 tokens typical) vs RAG (4K+ tokens)
- **Response variability**: Fixed 128 tokens vs 50-800 token range in production
- **Processing patterns**: Math reasoning vs creative writing stress different components

**Performance Impact**: Real data typically shows 25-40% higher latency variance and 2-5x difference in P99 metrics.

== Deploy the Granite Model

Before running benchmarks, we need to delete our Llama model deployment and deploy an IBM Granite model. This is due to access restrictions to the Llama model in HuggingFace and restricted time in our workshop. 

[source,console,role=execute,subs=attributes+]
----
# Delete the llama32 deployment
oc delete -k workshop_code/llama32 -n lls-demo

# Apply the granite-2b deployment
oc apply -k workshop_code/granite-2b/ -n lls-demo

.Ensure the model deployment is up and running fully before attempting a benchmark pipeline run.

== Verify your model deployment is ready

* Login to OpenShift AI and go to the **vllm** Data Science Project. Wait until the model fully deploys (green check) before continuing. 

image::granite-deployed-rhoai.png[Granite deployed on RHOAI]

You may then use your preferred method(s) to verify the successful deployment. We are not exposing an external route and the llm-hosting namespace within which we deployed the model has a network policy that blocks traffic from other namespaces. We will use a pod to curl the model.

== Execute the benchmark run

Execute the following command in the workbench terminal.

[source,console,role=execute,subs=attributes+]
----
guidellm benchmark \
  --target http://granite-2b-predictor.lls-demo.svc.cluster.local:8080/v1 \
  --processor "ibm-granite/granite-3.3-2b-instruct" \
  --data "prompt_tokens=512,output_tokens=128" \
  --max-seconds "30" \
  --rate "2" \
  --rate-type "sweep" \
  --max-requests "10" \
----

== Evaluate Output and Adjust GuideLLM Settings

GuideLLM captures the following metrics during a full sweep:

. **Requests per Second**: Total requests completed per second

. **Request concurrency**: average concurrent requests

. **Output token per second (mean)**: output tokens per second

. **Total tokens per second (mean)**: total (prompt + output) tokens per second

. **Request latency in ms (mean, median, p99)**: total end to end request latency

. **Time to First Token (mean, median, p99)**

. **Inter-Token Latency (mean, median, p99)**

. **Time per output token (mean, median, p99)**

See the complete https://github.com/neuralmagic/guidellm/blob/main/docs/metrics.md[metrics documentation^]. 

=== Reading Output

#### Top Section (Benchmark Info)

* Benchmark: The type of benchmark ran
- constant@x indicates the number of requests sent constantly to the model per second.
* Requests Made: How many requests issued (completed, incomplete or errors)
* Token Data
- Tok/Req: average tokens per request
- Tok Total: total number of input/output tokens processed

#### Bottom Section (Benchmark Stats)

* Mean
- Overall average
- Good for general performance overview

* Median
- Typical experience
- More stable, less skewed by outliers

* P99
- Worst-case real latency
- Essential for SLOs and user experience under load

=== Adjusting GuideLLM Settings

Depending on the results, you would try running GuideLLM a couple of different ways to see how the different controlled tests impact results.

== RAG (Retrieval-Augmented Generation) Simulation

Now, complete another benchmark run, this time changing the data-config parameter to simulate a RAG use case with a large context window. 

[source,console,role=execute]
----
guidellm benchmark \
  --target http://granite-2b-predictor.lls-demo.svc.cluster.local:8080/v1 \
  --processor "ibm-granite/granite-3.3-2b-instruct" \
  --data "prompt_tokens=4096,output_tokens=512" \
  --max-seconds "30" \
  --rate "2" \
  --rate-type "sweep" \
  --max-requests "10" \
----

Once complete, review the results and compare against the first run. Play around with the different settings while referencing the GuideLLM https://github.com/neuralmagic/guidellm?tab=readme-ov-file#configurations[configuration options].

=== Troubleshooting Performance Issues

#### High Latency Diagnosis
1. **TTFT > ITL**: Memory bandwidth or model loading bottleneck
2. **ITL >> TTFT**: Compute or batching inefficiency
3. **Both High**: Infrastructure under-sizing or configuration issues

#### Low Throughput Diagnosis  
1. **Compare synchronous vs throughput**: Reveals batching effectiveness
2. **Monitor GPU utilization**: Low utilization indicates non-GPU bottlenecks
3. **Analyze queue depths**: High queuing suggests insufficient parallelism

#### Inconsistent Performance Diagnosis
1. **P99 >> Median**: Resource contention or thermal throttling
2. **Variable between runs**: External factors or inadequate warm-up
3. **Degradation over time**: Memory leaks or resource exhaustion

== Summary

This activity demonstrated how to evaluate system performance using GuideLLM with a default vLLM configuration. By configuring vLLM more precisely or your chosen inference runtime, you can better align model serving with application needs—whether you’re optimizing for cost, speed, or user experience.
