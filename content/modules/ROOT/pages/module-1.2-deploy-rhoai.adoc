:imagesdir: ../assets/images

[#deploy-rhoai]
# Deploy vLLM on OpenShift AI with kserve and model car

1. Log into your provided OpenShift environment. 

We will use a kustomize deployment to deploy our model-car on vLLM in OpenShift AI. This helm chart is designed to be easily reusable and we recommend starting with this base for your deployment. In the previous module, the official Red Hat AI Inference Server image in the helm chart deployment on a standard OpenShift cluster. For the purposes of this training experience, we are using the vllm-kserve image on OpenShift AI. 

If you do use the RHAIIS image as seen in the previous module, you will run into the authentication requirements required for pulling the image and it would add unnecessary barriers to the training experience. 

You may view the helm chart at this link: https://github.com/redhat-ai-services/helm-charts/tree/main/charts/vllm-kserve.

NOTE: This is a great chart for you to use and contribute to in the future in your work. Consulting aims to keep it updated and as standard as possible to make customizing and deploying models with kserve and vLLM as easy as possible.

Let's take a look at our first provided custom values file for this workshop(etx-ai-presales/workshop_code/deploy_vllm/vllm_rhoai_custom_1/values.yaml):

[source,console,subs=attributes+]
----
deploymentMode: RawDeployment

fullnameOverride: granite-8b
model:
  modelNameOverride: granite-8b
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
  args:
    - "--max-model-len=130000"
    - "--enable-auto-tool-choice"
    - "--tool-call-parser=granite"
    - "--chat-template=/app/data/template/tool_chat_template_granite.jinja"

image:
  # -- The vLLM model server image
  image: 'quay.io/modh/vllm'

  # -- The tag or sha for the model server image
  tag: rhoai-2.22-cuda-5e9c649953464aa3a668aba1774e42fc933f721b

  # -- The vLLM version that will be displayed in the RHOAI Dashboard.  If not set, the appVersion of the chart will be used.
  runtimeVersionOverride: ""
----

You may substitute the modelcar for a different model and adjust the arguments as desired using vLLM standard args: https://docs.vllm.ai/en/stable/configuration/engine_args.html#named-arguments. 

View our available modelcars here: https://quay.io/repository/redhat-ai-services/modelcar-catalog?tab=tags

If you choose to use a different model you'll need to ensure to change the tool-call-parser and the chat-template fields appropriately. 

## Understanding Chat Templates and Tool Calling

When using different models, you need to update these key arguments:

== Chat Templates
Chat templates format conversations for the model. Each model expects a specific format based on its training, and may support multiple different template types:

* **Jinja2 Templates**: Nearly all HF chat models ship with this template type. It's declaritive, portable and can be bundled in the model card and config without shipping additional code.
* **Pythonic Templates**: Requires shipping actual Python code. A bit less portable and requires more maintenance.

All models almost always ship with the jinja2 format template. Some models also include the pythonic templates but it isn't guaranteed. 

NOTE: For production deployments and advanced pipelines, the pythonic template is considered the standard and is recommended over the jinja2 template.

== Tool Call Parsers
For function calling models, the parser extracts structured tool calls from model output:

* `--tool-call-parser=granite` - For IBM Granite models

**Quick Reference:**
- Chat templates: https://docs.vllm.ai/en/stable/features/tool_calling.html
- Tool parsers: https://docs.vllm.ai/en/latest/cli/index.html#-tool-call-parser
- Use `pythonic` chat template when available for simpler configuration

## Deploy the vLLM model car chart

### Add the redhat-ai-services helm chart repository

[source,console,role=execute,subs=attributes+]
----
helm repo add redhat-ai-services https://redhat-ai-services.github.io/helm-charts/
helm repo update redhat-ai-services
----

### Deploy helm chart in llm-hosting namespace

Run the following commands to deploy our vLLM-kserve helm chart. The chart version we are using is published, but we will be deploying it from our cloned repository so that we may view files and make any changes if desired.

OPTIONAL: Before deploying, adjust your `values.yaml` file as you desire as described in above sections or use the provided starting file.

Switch to the **llm-hosting** project: 

[source,console,role=execute,subs=attributes+]
----
oc project llm-hosting
----

NOTE: Ensure you're in the **etx-ai-presales** cloned repository in your local environment before continuing.

[source,console,role=execute,subs=attributes+]
----
helm install granite-8b redhat-ai-services/vllm-kserve --version 0.5.9 \
  -f workshop_code/deploy_vllm/vllm_rhoai_custom_1/values.yaml 
----

## Verify deployment

It will take about 10 minutes for the model deployment to be ready.

* Login to OpenShift AI and go to the **LLM HOST** Data Science Project. Wait until the model fully deploys (green check) before continuing. 

image::granite-deployed-rhoai.png[Granite deployed on RHOAI]

You may then useyour preferred method(s) to verify the successful deployment. We are not exposing an external route and the llm-hosting namespace within which we deployed the model has a network policy that blocks traffic from other namespaces. We will use a pod to curl the model.

### Curl Models

Again, in the OpenShift console terminal:

[source,sh,role=execute]
----
oc -n llm-hosting run curl --image=curlimages/curl --restart=Never -it --rm -- \
  curl -s http://granite-8b-predictor.llm-hosting.svc.cluster.local:8080/v1/models
----

## Conclusion

We now have our model car deployed and will move on to model optimization and evaluation!