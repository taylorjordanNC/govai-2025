:imagesdir: ../assets/images

[#agentic-llamastack]
= Getting Started with Llama Stack

In this module, you'll get a better understanding of Llama Stack, how its deployed in our OpenShift AI cluster, and learn how it connects to models and tools. We'll explore this foundational tool for building agentic AI applications on OpenShift AI.

IMPORTANT: Before you begin, we will be adjusting a Llama Stack configmap manually during this exercise. This file is currently managed by GitOps. Please ensure, within ArgoCD, that the llama-stack app auto-sync is **disabled**. You can access ArgoCD via the OpenShift console top navigation bar.

== What you'll accomplish

* Learn about the Llama Stack operator and architecture
* Verify your Llama Stack deployment is working
* Confirm granite model connectivity (already pre-configured)
* Explore the Llama Stack playground interface
* Test basic agentic AI functionality and prepare for hands-on activities

== Verify LlamaStack Service

We'll verify that Llama Stack is already deployed and accessible in your `lls-demo` namespace.

. Check that LlamaStack pods are running in your namespace
+
[source,bash,options="wrap",role="execute"]
----
oc -n lls-demo get pods | grep llamastack
----

. Verify the LlamaStack service is available
+
[source,bash,options="wrap",role="execute"]
----
# Test direct connection to the service
curl -s http://llamastack-service.lls-demo.svc.cluster.local:8321/v1/version
----

. Check what models are available
+
[source,bash,options="wrap",role="execute"]
----
curl -s http://llamastack-service.lls-demo.svc.cluster.local:8321/v1/models
----

== Understanding Llama Stack

https://llama-stack.readthedocs.io/en/latest/[Llama Stack,window=_blank] is the open-source framework for building generative AI applications. We are going to deploy LlamaStack using the Operator and then take a look around using the client CLI and the playground UI.

**Key Benefits for Enterprise:**
* **Unified API** - Single interface for models, agents, and tools
* **Built-in Safety** - Enterprise-grade guardrails and content filtering
* **Tool Integration** - Secure connections to live systems via MCP servers
* **Auditability** - Complete logging and observability

https://github.com/llamastack/llama-stack-k8s-operator[Github Upstream link,window=_blank].

The Llama Stack K8s Operator is a Kubernetes operator that automates the deployment and management of Llama Stack servers in Kubernetes environments. It provides a declarative way to manage AI model serving infrastructure. It is:

. Kubernetes native and follows standard operator development pattern.
. Supports Ollama and vLLM inference providers
. Allows configuring and managing LlamaStack servers and client instances.

NOTE: Red Hat provides a downstream version of this operator which will be GA at the end of the year for our 3.0 release. However, to ensure we're using the latest upstream features ahead of GA, we have used the upstream operator for this workshop.

== DataScienceCluster Custom Resource

. Check the LlamaStack controller manager pod is running in the **llama-stack-k8s-operator-controller-manager** Namespace
+ 
image::llama/images/llama-stack-controller-manager2.png[LlamaStack Controller Manager Pod, 800]

== LlamaStackDistribution Custom Resource

The `LlamaStackDistribution` is the main custom resource that defines how a Llama Stack server should be deployed. It allows you to specify:

* **Server Configuration**: Which distribution to use (Ollama, vLLM, etc.)
* **Container Specifications**: Port, environment variables, resource limits

You may have many LlamaStackDistribution instances deployed in a cluster.


== Granite Model Configuration

You may already have a Granite model deployed from a previous module. Please uninstall or remove this deployment.

[source,console,role=execute,subs=attributes+]
----
helm uninstall granite-8b
----

Let's deploy the Granite model again. We will use a separate kustomize deployment for these exercises due to tool-calling behavior issues with the Granite 3.3 deployment in this workshop.

Ensure you are still in the `etx-ai-presales` directory locally:

[source,console,role=execute,subs=attributes+]
----
oc apply -k workshop_code/granite-8b -n llm-hosting 
----

Your Llama Stack in the `lls-demo` namespace is already configured to access this model.

== Set Up Data Science Workbench

=== Create a workbench

. Login to OpenShift AI and select the `lls-demo` data science project.

. We are going to `Create a workbench` using the following parameters:

    Name: agent-tools
    Image Selection: Standard Data Science
    Version: 2025.1 (select the latest version)

. Leave all the other fields as defaults.

. Select `Create workbench`.

. Once the workbench is running open it in your browser.

. Open a Terminal from the Launcher tab.

== Verify Model Connection

. In the JupyterLab interface, select the `Terminal` from the Launcher tab.

+
Your Llama Stack deployment is already pre-configured to connect to granite-3dot2-8b-instruct. Let's verify this connection is working and see what models are available.
+
. Check that Llama Stack can see the granite-3dot2-8b-instruct model
+
[source,bash,options="wrap",role="execute"]  
----
curl -s http://llamastack-service.lls-demo.svc.cluster.local:8321/v1/models
----
+
You should see granite-8b listed in the available models.

=== Example LlamaStackDistribution

Here is a very basic configuration. Note that the RHOAI distribution is named **rh-dev** and the upstream is named **remote-vllm**

[source,yaml,options="wrap"]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
   name: llamastack-with-config
spec:
   replicas: 1
   server:
     distribution:
       name: rh-dev # remote-vllm (upstream)
     containerSpec:
       port: 8321
     userConfig:
        # reference to the configmap that contains Llama stack configuration.
       configMapName: llama-stack-config 
----

// NOTE: We maintain a https://github.com/eformat/distribution-remote-vllm[build of LlamaStack,window=_blank] that https://github.com/redhat-ai-services/etx-agentic-ai/blob/main/infra/applications/llama-stack/base/llama-stack.yaml#L39[pins the image version,window=_blank] so we can ensure stability whilst the upstream rapidly changes. We expect to use the **rh-dev** distribution once <<Using the DataScienceCluster Resource to configure the LlamaStack Operator>> is resolved.

== Using ConfigMap for run.yaml Configuration

The operator supports using ConfigMaps to store the https://github.com/llamastack/llama-stack/blob/main/llama_stack/distributions/nvidia/run.yaml[run.yaml,window=_blank] configuration file. 

* **Centralized Configuration**: Store all Llama Stack settings in one place
* **Dynamic Updates**: Changes to the ConfigMap automatically restart pods to load new configuration
* **Environment-Specific Configs**: Use different ConfigMaps for different environments

=== ConfigMap Basic Example

. Here is a basic example of the **run.yaml** config provided to our LlamaStack deployment that has just the Tavily Web Search provider configured.
+
[source,yaml,options="wrap"]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: llamastack-config
data:
  run.yaml: |
    # Llama Stack configuration
    version: '2'
    image_name: vllm
    apis:
    - tool_runtime
    providers:
      tool_runtime:
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_API_KEY}
          max_results: 3
    tools:
      - name: builtin::websearch
        enabled: true
    tool_groups:
    - provider_id: tavily-search
      toolgroup_id: builtin::websearch
    server:
      port: 8321
----

. Verify Llama Stack deployment status in your local terminal. The workbench does not have access to all of the resources.

+
[source,bash,options="wrap",role="execute"]
----
oc -n lls-demo get deploy,pod,svc | grep llamastack
----

. Check Llama Stack pod is running OK, check its logs
+
image::llama/images/llama-stack-basic-pod.png[LlamaStack Basic Pod, 800]

. Install the **llama-stack-client** in your workbench.
+
[source,bash,options="wrap",role="execute"]
----
pip install llama-stack-client==0.2.12 fire
----
+

. Set up direct connection to the Llama Stack service in your workbench
+
Since both your workbench and Llama Stack are running in the same cluster, you can connect directly:
+
[source,bash,options="wrap",role="execute"]
----
export LLAMA_STACK_URL="http://llamastack-service.lls-demo.svc.cluster.local:8321"
----
+

. Check the connection by listing the version - ideally we match client and server versions
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client --endpoint http://llamastack-service.lls-demo.svc.cluster.local:8321 inspect version
----

You should see the following output:

[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://llamastack-service.lls-demo.svc.cluster.local:8321/v1/version "HTTP/1.1 200 OK"
VersionInfo(version='0.2.12')
----


. If you need help with or want to peruse the client commands, take a look at
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client --help
----

. Now list the providers -
+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client --endpoint http://llamastack-service.lls-demo.svc.cluster.local:8321 providers list
----
+
[source,bash,options="wrap"]
----
INFO:httpx:HTTP Request: GET http://llamastack-service.lls-demo.svc.cluster.local:8321/v1/providers "HTTP/1.1 200 OK"
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ API          ┃ Provider ID            ┃ Provider Type                  ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ inference    │ vllm                   │ remote::vllm                   │
│ inference    │ sentence-transformers  │ inline::sentence-transformers  │
│ inference    │ vllm-safety            │ remote::vllm                   │
│ agents       │ meta-reference         │ inline::meta-reference         │
│ eval         │ meta-reference         │ inline::meta-reference         │
│ datasetio    │ huggingface            │ remote::huggingface            │
│ datasetio    │ localfs                │ inline::localfs                │
│ vector_io    │ faiss                  │ inline::faiss                  │
│ safety       │ llama-guard            │ inline::llama-guard            │
│ scoring      │ basic                  │ inline::basic                  │
│ scoring      │ llm-as-judge           │ inline::llm-as-judge           │
│ scoring      │ braintrust             │ inline::braintrust             │
│ telemetry    │ meta-reference         │ inline::meta-reference         │
│ tool_runtime │ tavily-search          │ remote::tavily-search          │
│ tool_runtime │ rag-runtime            │ inline::rag-runtime            │
│ tool_runtime │ model-context-protocol │ remote::model-context-protocol │
└──────────────┴───────────────┴───────────────────────------------------┘
----

. Set up an external route to access the Swagger documentation
+
IMPORTANT: These commands require cluster admin permissions. Run them from your **local terminal** (not from within the workbench) where you have admin access.


[source,bash,role="execute"]
----
# From your local terminal with admin access:
oc -n lls-demo expose svc/llamastack-service --name=llamastack-external

# Get the external URL for the Swagger docs
EXTERNAL_HOST=$(oc -n lls-demo get route llamastack-external -o jsonpath='{.spec.host}')
echo "Swagger UI: http://${EXTERNAL_HOST}/docs"
echo "API Base URL: http://${EXTERNAL_HOST}"

# Test that the external API is working
curl -s http://${EXTERNAL_HOST}/v1/version
----

TIP: You can now access the interactive Swagger API documentation in your browser using the external URL. This shows your actual configured models, tools, and providers, and allows you to test API calls directly.


=== LlamaStack User Interface

Access the playground through your cluster's route:

image::llama/images/llama-stack-playground2.png[LlamaStack Playground UI, 800]

. We can Chat with the LLM - this calls the **/v1/chat/completion** endpoint that we can find in the OpenAI docs for the vLLM served model. You can prompt it to check the connection is working. Ask the LLM a more complex question such as:
+
[source,bash,options="wrap",role="execute"]
----
What is LlamaStack ?
----
to see if it contains that information.
+
image::llama/images/llama-stack-playground-hello.png[LlamaStack Playground Hello, 800]

== Configure Web Search with Tavily API

To enable the web search functionality in Llama Stack, you'll need to configure a Tavily API key. This allows your AI agent to search the internet for real-time information.

=== Get a Tavily API Key

. Visit https://app.tavily.com/[Tavily^] and sign up for a free account
. Navigate to the "API Keys" section  
. Copy your API key (starts with `tvly-`)

=== Apply the Tavily Secret

From your **local terminal** (not the workbench), apply the Tavily API key to your cluster:

. Create the secret with your API key:
+
[source,bash,options="wrap",role="execute"]
----
# Replace YOUR_TAVILY_API_KEY with your actual key from Tavily
oc create secret generic tavily-api-key \
  --from-literal=TAVILY_API_KEY="YOUR_TAVILY_API_KEY" \
  --namespace=lls-demo
----

. Verify the secret was created:
+
[source,bash,options="wrap",role="execute"]
----
oc get secret tavily-api-key -n lls-demo
----

=== Update the Llama Stack Configuration

. Update the Llama Stack ConfigMap to fix the YAML syntax for the Tavily configuration:
+
[source,bash,options="wrap",role="execute"]
----
# Get the current config and edit it
oc edit configmap llama-stack-config -n lls-demo
----

. In the editor, find the `tavily-search` provider configuration and ensure it looks like this (fix any `config: {}` to just `config:` and then uncomment the api_key and max_results fields:
+
[source,yaml,options="wrap"]
----
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_API_KEY}
          max_results: 3
----

. Save and exit the editor (`:wq` in vim or `Ctrl+X, Y, Enter` in nano)

=== Inject the Secret into the Deployment

. Add the environment variable to the Llama Stack deployment:
+
[source,bash,options="wrap",role="execute"]
----
oc patch deployment llamastack -n lls-demo --type='json' -p='[
  {
    "op": "add", 
    "path": "/spec/template/spec/containers/0/env/-", 
    "value": {
      "name": "TAVILY_API_KEY",
      "valueFrom": {
        "secretKeyRef": {
          "name": "tavily-api-key",
          "key": "TAVILY_API_KEY"
        }
      }
    }
  }
]'
----

. Wait for the pod to restart and verify it's running:
+
[source,bash,options="wrap",role="execute"]
----
oc get pods -n lls-demo | grep llamastack
----
+

. Verify no errors in the logs:
+
[source,bash,options="wrap",role="execute"]
----
oc logs deployment/llamastack -n lls-demo --tail=10
----

=== Test Web Search Functionality

Now return to the Llama Stack playground and test the web search capability:

. Select the **Agent-based** processing mode and the **websearch** built-in tool.

. Try the prompt:
+
[source,bash,options="wrap",role="execute"]
----
What is the weather today in Brisbane ?
----

You should now see the agent use the web search tool to find current weather information!


=== Example: Llama Stack Observability Integration

NOTE: This is an **example** showing how Llama Stack can integrate with enterprise observability stacks. This is not required for the core workshop activities but demonstrates production-ready monitoring capabilities.

Llama Stack can integrate with OpenShift's observability infrastructure for comprehensive monitoring and tracing. In this example setup, traces are sent from LlamaStack via OpenTelemetry (OTEL) to a Tempo sink endpoint, which can then be viewed in OpenShift's built-in Observe > Traces dashboard.

image::llama/images/llama-stack-sno.png[LlamaStack Telemetry, 300]

==== Example Configuration

The telemetry configuration in the ConfigMap shows how observability is configured:

[source,yaml,options="wrap"]
----
      telemetry:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          service_name: ${env.OTEL_SERVICE_NAME:=llama-stack}
          sinks: ${env.TELEMETRY_SINKS:=console, sqlite, otel_metric, otel_trace}
          otel_exporter_otlp_endpoint: ${env.OTEL_EXPORTER_OTLP_ENDPOINT:=}
          sqlite_db_path: ${env.SQLITE_DB_PATH:=~/.llama/distributions/remote-vllm/trace_store.db}
----

==== Viewing Traces

When you use the playground with tool calls (such as websearch, MCP servers, or ReAct agents), you can generate traces that appear in OpenShift's observability dashboard:

image::llama/images/llama-stack-traces1.png[LlamaStack Traces, 800]
+
image::llama/images/llama-stack-traces2.png[LlamaStack Traces, 800]

This integration provides valuable insights into:
- **Agent reasoning chains**: See how the LLM decides which tools to use
- **Tool execution timing**: Monitor performance of external API calls  
- **Error tracking**: Debug failed tool calls or agent interactions
- **Usage patterns**: Understand how agents are being utilized in production


=== Using the DataScienceCluster Resource to configure the LlamaStack Operator

This section is for **information only** and should be supported in 2.23+ of RHOAI. (we are on a v2.22 RHOAI cluster)

[NOTE]
====
**VERSION 2.22 DOES NOT HAVE USERCONFIG MAP OVERRIDE SO DO NOT USE DSC YET - SCHEDULED FOR 2.23 **

With the latest version of RHOAI 2.22.0+ we can use the built in DSC (Data Science Cluster) mechanism to deploy the operator.

1. Ensure the DataScienceCluster resource has the **llamastackoperator** component as **Managed**

2. First, navigate to the correct project via
+
[source,bash,options="wrap",role="execute"]
----
oc project agent-demo
----

3. Check the DataScienceCluster resource
+
[source,bash,options="wrap",role="execute"]
----
oc get dsc -o yaml
----

4. Finally, review the DataScienceCluster resource to ensure the **llamastackoperator** component is set to **Managed**
+
[source,yaml,options="wrap"]
----
apiVersion: datasciencecluster.opendatahub.io/v1
kind: DataScienceCluster
metadata:
  name: default-dsc
spec:
  components:
    ...
    llamastackoperator:
      managementState: Managed
    ...
status: {}
----

. Check the LlamaStack controller manager pod is running in the **redhat-ods-applications** Namespace
+ 
image::llama/images/llama-stack-controller-manager.png[LlamaStack Controller Manager Pod, 800]


