:imagesdir: ../assets/images

[#agentic-conclusion]
= Module 2 Wrap-up: Selling Agentic AI

You've now seen how to deploy Llama Stack on OpenShift and connect it to real systems. Here's what you need to know to talk to customers about this.

== What You Just Built

* **Llama Stack deployment** on OpenShift with standard Kubernetes patterns
* **Tool integrations** using MCP servers (web search, OpenShift APIs, Slack)  
* **Natural language interface** for infrastructure operations
* **Multi-system workflows** triggered by simple prompts

== Key Takeaways

**It's not just another chatbot**
* Executes real tasks across multiple systems
* Uses reasoning to handle complex, multi-step operations
* Integrates with existing infrastructure instead of replacing it

**Runs on your platform** 
* Uses standard K8s/OpenShift deployment patterns
* Follows existing RBAC and security policies
* No external API dependencies for core functionality

**Connects to anything**
* MCP protocol provides standard way to add new tools
* Works with REST APIs, databases, enterprise apps
* You control what systems the AI can access

**Practical ROI**
* Reduces time spent on routine operational tasks
* Lets non-experts safely query production systems  
* Consolidates multiple tools behind single interface

== How It Works

The architecture is straightforward:

* **Llama Stack** - Orchestrates models and tools via standard APIs
* **MCP Servers** - Connect to external systems (you saw OpenShift and Slack)
* **ConfigMaps** - Define which tools and models are available
* **OpenShift** - Handles deployment, security, and scaling like any other app

== Next Steps

**For pilot projects:**
* Pick 2-3 routine tasks that involve multiple tools
* Start with read-only operations (monitoring, reporting)
* Expand to controlled actions once you're comfortable

**Technical setup:**
* K8s cluster (you can start with a small deployment)
* Identify existing APIs that would benefit from natural language access
* Build or deploy MCP servers for your specific tools

**Resources:**
* Llama Stack docs: https://llama-stack.readthedocs.io/
* MCP protocol: https://modelcontextprotocol.io/
