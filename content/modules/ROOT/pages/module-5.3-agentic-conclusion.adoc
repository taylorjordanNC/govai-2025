:imagesdir: ../assets/images

[#agentic-conclusion]
= Module 5 Wrap-up: Selling Agentic AI

You've now seen how to deploy Llama Stack on OpenShift and connect it to real systems. Here's what you need to know to talk to customers about this.

== What You Just Built

* **Llama Stack deployment** on OpenShift with standard Kubernetes patterns
* **Tool integrations** using MCP servers (web search, OpenShift APIs, Slack)  
* **Natural language interface** for infrastructure operations
* **Multi-system workflows** triggered by simple prompts

== Key Points for Customer Conversations

**It's not just another chatbot**
* Executes real tasks across multiple systems
* Uses reasoning to handle complex, multi-step operations
* Integrates with existing infrastructure instead of replacing it

**Runs on your platform** 
* Uses standard OpenShift deployment patterns
* Follows existing RBAC and security policies
* No external API dependencies for core functionality

**Connects to anything**
* MCP protocol provides standard way to add new tools
* Works with REST APIs, databases, enterprise apps
* You control what systems the AI can access

**Practical ROI**
* Reduces time spent on routine operational tasks
* Lets non-experts safely query production systems  
* Consolidates multiple tools behind single interface

== How It Works

The architecture is straightforward:

* **Llama Stack** - Orchestrates models and tools via standard APIs
* **MCP Servers** - Connect to external systems (you saw OpenShift and Slack)
* **ConfigMaps** - Define which tools and models are available
* **OpenShift** - Handles deployment, security, and scaling like any other app

== Customer Questions You Can Now Answer

**"How is this different from ChatGPT?"**
It executes tasks instead of just providing text. You saw it get pod logs and post them to Slack.

**"What systems can it connect to?"** 
Anything with an API. MCP servers make integration standardized - you write one plugin and it works across all your AI agents.

**"Is it secure?"**
It follows standard OpenShift RBAC. The AI can only access what you explicitly configure.

**"How much does it cost?"**
No per-query charges for internal operations. Uses your existing OpenShift infrastructure.

== Common Objections

**"Our team isn't ready for AI"**
The interface is natural language - simpler than learning new CLI tools.

**"We already have automation tools"**
This works with your existing tools rather than replacing them. Think of it as a universal interface.

**"AI isn't reliable enough for production"**
All actions are logged and auditable. You can configure approval workflows for sensitive operations.

== Next Steps

**For pilot projects:**
* Pick 2-3 routine tasks that involve multiple tools
* Start with read-only operations (monitoring, reporting)
* Expand to controlled actions once you're comfortable

**Technical setup:**
* OpenShift AI cluster (you can start with a small deployment)
* Identify existing APIs that would benefit from natural language access
* Build or deploy MCP servers for your specific tools

**Resources:**
* Llama Stack docs: https://llama-stack.readthedocs.io/
* MCP protocol: https://modelcontextprotocol.io/
* OpenShift AI: Contact your Red Hat team for deployment planning
