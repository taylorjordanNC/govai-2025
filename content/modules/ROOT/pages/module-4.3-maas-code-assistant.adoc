:imagesdir: ../assets/images

[#code-asst]
= Coding with AI

Welcome to the next stage of our journey!

Now, you become an AI application developer. Your job: build smarter, faster - with a little help from a language model.

== Why Code Assistants Matter

You're not just here to write code - you're here to collaborate with an **AI-powered teammate**.

Modern code assistants can:

* **Understand Full Project Context**: Assistants like Continue, GitHub Copilot, and Cursor can read across files and directories, understanding how your code fits together.

* **Edit code with Natural Language**: You can describe a change—like "add error handling here" or "convert this to async"—and assistants like Continue, Cursor, or CodeWhisperer will apply it intelligently.

* **Proactively Suggest Improvements**: They catch bugs, highlight inefficiencies, and recommend best-practice patterns.

* **Automate Tedious Tasks**: From generating unit tests and docstrings to scaffolding boilerplate or renaming variables across a project—they take care of the grunt work so you can focus on logic and design.

These assistants don’t just write code - they understand your project and help you build better software faster. And the best part? Most of them work seamlessly inside popular IDE's like Visual Studio Code, so you don’t need to switch tools to get started.

Let's get set up.

== Step 1: Setting Up Your Cloud Integrated Developer Environment (IDE) with OpenShift Dev Spaces

As a developer, your environment should be fast, consistent, and ready to go for your work. That is what OpenShift Dev Spaces delivers: instant access to pre-configured, containerized workspaces — all running securely on your OpenShift cluster.

You could also easily leverage our exposed model API endpoint in your local environment as a developer. However, your company has specific restrictions and require you to work within the OpenShift cluster environment.

=== Access OpenShift Dev Spaces Console

Your platform engineer has provided you a direct URL to the Dev Spaces console to begin your work.

*  Navigate to console: https://devspaces.{openshift_cluster_ingress_domain}

We have a VSCode workspace ready for your use, with the required dependencies. As a developer, you can also create your own workspace without needing to install or configure anything locally. There may however, be some restrictions put in place by your admin around resource limitations or only using approved devfiles (yaml files used to instantiate new workspaces). 

* Open the `Workspaces` tab:

image::code/workspaces-tab.png[width="75%"]

*  Launch the pre-created workspace by clicking `Open`. You may need to wait a moment for the workspace to be ready.

image::code/workspace_preset.png[width="75%"]

After a moment, you'll see the VSCode interface running in your browser. If prompted, click "Trust" when asked about the authors. We're friendly, I promise! 

image::code/vscode_trust.png[width="75%"]

Inside your workspace, you'll find a cloned GitHub repository. This is where you will build and refine. The only thing we need to configure is our code extension.

image::code/file_explorer_repo.png[width="75%"]

Before we dive into the code, let's meet **Continue**.

== Step 2: Add Continue, Your Coding Teammate

**Continue** is an open-source AI code assistant that integrates seamlessly into VS Code. Unlike traditional code completion tools, Continue provides an interactive chat interface where you can have natural conversations about your code. You can ask Continue to perform actions like:

**“Add logging to this function”**

**“Generate a unit test for this file”**

**“Refactor this into smaller components”**

**“Explain what this code does”**

Continue will execute these requests directly in your editor, with full awareness of your codebase and project context.

What makes Continue particularly powerful is its flexibility - it supports custom model endpoints, making it perfect for connecting to your private enterprise models. As an **open-source** solution, Continue gives you complete control over your AI coding workflow. 

NOTE: You could use any preferred code assistant in this setup, like Cursor, Roo Code, or others. For the purposes of this hands-on experience and with the constraints of our workshop environment, we will be using Continue.

=== Install Continue

Select the bottom navigation item on the left-hand side to open up the extensions marketplace.

image::code/extensions_tab.png[width=100%]

In the search bar, search for **Continue**.

image::code/continue.png[width="75%"]

Click on the arrow next to **Install Pre-Release** and instead select **Install Release Version** on the Continue extension.

image::code/install_release_version.png[width="75%"]

You will get a trust verification message. Select **Trust Publisher & Install**.

image::code/trust_continue.png[width="75%"]

Once installed, click on the arrow next to `Uninstall -> Install Specific Version` as seen below. We are going to select a specific version to install since things move so quickly around here!

Click **Install Specific Version...**

image::code/install_specific_version.png[width="75%"]

You will see a drop-down appear with different versions. Please select **v1.0.21**.

Once installed, select "Reload Window" to restart the extension.

image::code/reload_window.png[width="75%"]

You've now installed Continue - next, let's connect it to your private LLM.

== Step 3: Connect Continue to Your Granite Model

Navigate to the **Continue sidebar icon** in the left-hand side navigation panel:

image::code/continue_sidebar.png[width="75%"]

We will do two things in this module:

**1.** Connect to our Granite model within our company's MaaS platform
**2.** Use our model's "brain" to help us understand and deploy a fun game, and then work on an impactful Kubernetes deployment.

In order to connect our model to the Continue code extension we must provide the extension our model's endpoint URL and API key from our MaaS application in the previous module.

NOTE: If needed, go back to the developer portal and the application created in the previous module to retrieve the credentials: https://maas.{openshift_cluster_ingress_domain}

=== Enter Connection Details

Click on `Select Model` and then the small settings icon in the **Models** pop up.

image::code/open_config.png[width="75%"]

This will open the `config.yaml` file. Delete the file contents and replace with the following:

[source,yaml,role="execute"]
----
name: Local Assistant
version: 1.0.0
schema: v1
models:
  - name: Granite-3.2-8b-instruct
    provider: openai
    model: "granite-3dot2-8b-instruct"
    apiBase: "https://granite-3dot2-8b-instruct-maas-apicast-production.{openshift_cluster_ingress_domain}:443/v1"
    apiKey: "YOUR_API_KEY"
context:
  - provider: code
  - provider: docs
  - provider: diff
  - provider: terminal
  - provider: problems
  - provider: folder
  - provider: codebase
----

IMPORTANT: Replace the `apiBase` URL with your actual MaaS endpoint URL and `YOUR_API_KEY` with the API key from your MaaS application. 

For example:

* **apiBase**: `https://granite-model-maas.apps.example.com:443/v1` (Ensure you retain the `v1` at the end of the URL)
* **apiKey**: `your-actual-api-key-here`

Reference the complete configuration documentation here: https://docs.continue.dev/reference[Continue Documentation]

When the model is properly configured, you will see the model name, `Granite-3.2-8b-instruct` in the Continue chat sidebar.

image::code/model_dropdown.png[width="75%"]

Go ahead - test it out and chat a bit!

== You're Ready to Code with AI

You've now:

* Set up a cloud IDE
* Installed Continue and configured it to connect to your private Granite model
* Set up an AI assistant that can refactor, edit and explain your code!

Next, you will use Continue to help you develop a little fun game to get warmed up. 