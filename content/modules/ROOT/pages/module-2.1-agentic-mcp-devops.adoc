:imagesdir: ../assets/images

[#agentic-mcp-devops]
= Hands-on: System Administration with Agentic AI 

Now that you have Llama Stack configured, let's put it to work! In this hands-on module, you'll step into the shoes of a **DevOps engineer** using agentic AI for real infrastructure management tasks.

You'll connect Llama Stack to live OpenShift resources and external systems like Slack, then use natural language to monitor, troubleshoot, and manage your infrastructure.

== Why Agentic AI for Admins?

Think of agentic AI as your copilot for cluster operations - ready to answer complex infrastructure questions using real-time telemetry and reasoning. 

Instead of navigating complex dashboards, YAML, and shell scripts, you'll try something new:

* “What’s consuming the most memory in our dev cluster right now?”

* “Were there any pod crashes in the last 30 minutes?”

* “Why did that Job fail this morning?”

These natural-language prompts get translated into real queries and results. The outcome? Faster issue resolution, increased system transparency, and lower barrier to operational insights for both technical and non-technical users.

That's an example of the power of agentic AI.

== Meet Your New Toolkit

In this lab, we'll use a preconfigured stack that includes:

* **https://github.com/meta-llama/llama-stack[Llama Stack]** - a flexible open-source framework developed by Meta to simplify the creation and deployment of advanced AI applications.
** Llama Stack provides a unified API layer for inference, RAG, agents, tools, safety, evals and telemetry.
** **Why It Matters for Enterprises**:
*** Codifies best practices across Gen AI tools
*** Enables reproducible, explainable, and extensible AI workflows
*** In short, it lets you focus on value creation, not toolchain assembly.

IMPORTANT: You could also use frameworks like LangChain or CrewAI instead of Llama Stack with OpenShift AI. All of these tools help you build agentic AI workflows with reasoning, tool use, and orchestration. Llama Stack is Red Hat's recommended, and supported, framework.

* **OpenShift MCP Server**  - allows our LLM agent to interact with our live OpenShift cluster. This MCP server is namespace-scoped. We will use it to interact with the OpenShift resources within the `lls-demo` namespace.

== View Your Deployment

1. Go to the OpenShift console (link provided to you when you logged into your environment). 

2. Now, select the `Developer` perspective.

+
image:llama/dev_perspective.png[width="75%"]
+

3. In case you are not in our specific project where the Llama Stack resources are deployed, search for the `lls-demo` namespace:

+
image:llama/find-namespace.png[width="75%"]
+

4. Select the `Topology` tab in the navigation bar as seen above.

In the Topology view, you will see four pods:

* **Llama Stack**: core server that connects Gen AI models to real-world tools and services. Our Llama Stack server handles the complex orchestration of turning natural language requests into real API calls, tool calls, and responses while maintaining context and security.
* **OCP MCP Server**: an MCP Server with tools to help our model interact with and understand OpenShift.
* **Llama Stack Playground**: A streamlit UI to interact with the system.

image::llama/see_topology.png[width="75%"]

Feel free to poke around and explore the deployment.

5. Select the Llama Stack playground hyperlink to open the UI.

image:llama/playground_link.png[width="75%"]

Now you will see the "playground" user interface. This application was created in the upstream project for the purposes of demonstration and experimentation and is **not** a supported component of our downstream OpenShift AI product.

image::llama/playground_ui.png[width="75%"]

== Configure the AI Agent

Within the UI application you'll find a familiar chat interface with some selection options on the left-hand side.

1. Select our model from the drop down

+
[.bordershadow]
image::llama/model_selection.png[width="75%"]
+

2. Set `Processing mode` -> `Agent-based`, giving us access to the tools we have configured via the MCP servers.

+
image::llama/agent_selection.png[width="75%"]
+

3. Enable the OpenShift MCP tool group.

+
image::llama/mcp_servers.png[width="75%"]
+

4. Once the MCP server is selected, you can peruse the active tools available.

+
image:llama/active_tools.png[width="75%"]
+

Everything else can remain unchanged.

== Investigate our OpenShift Resources

The active tools information will give you guidance into how to interact with the model in chat to activate the tool calls correctly.

NOTE: Our Llama Stack deployment is namespace-scoped. Therefore, in this activity, we will only be able to interact with the OpenShift resources within the `lls-demo` namespace containing the Llama Stack server and playground.

In the chat, enter:

[source,console,role=execute,subs=attributes+]
----
List all pods in the lls-demo namespace.
----

Response output will vary. But you will see it activate the tool, and give you a response. Something like this:

image::llama/ocp_response_example.png[width="75%"]

Let's try something else:

[source,console,role=execute,subs=attributes+]
----
Get logs for the <ocp-mcp-server-pod-name> pod in the lls-demo namespace.
----

IMPORTANT: You will need to replace the `<ocp-mcp-server-pod-name>` with the actual pod name. You can find the pod name from the response to the `list all pods` prompt.

You will again see that the associate tool is activated, and the model will then generate a response from the context provided by the tool call.

Feel free to experiment further with the tools available.

NOTE: We are using a small model, which is not optimal for agentic AI performance in production use cases. For demos and non-critical work, it can be quite impressive! However, some responses may be incomplete or inconsistent, and the model may hallucinate or misinterpret results if the tool output is vague or malformed or if we are asking it to engage with multiple MCP servers (like in this workshop!). The demonstration is meant to highlight the potential of natural language interfaces for interacting with infrastructure, and how emerging tools like Llama Stack and MCP can reduce the barrier to entry for understanding system behavior and save valuable time and effort.

== Summary: What You Did

In this module, you:

* Acted as a DevOps engineer using AI for cluster resource insight
* Integrated your own LLM with a tool-using agent.
* Explored OpenShift resources with natural language

You just used AI to reduce operational complexity and speed up workflows! 