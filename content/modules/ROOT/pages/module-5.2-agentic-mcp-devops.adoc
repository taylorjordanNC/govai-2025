:imagesdir: ../assets/images

[#agentic-mcp-devops]
= Hands-on: System Administration with Agentic AI 

Now that you have Llama Stack configured, let's put it to work! In this hands-on module, you'll step into the shoes of a **DevOps engineer** using agentic AI for real infrastructure management tasks.

You'll connect Llama Stack to live OpenShift resources and external systems like Slack, then use natural language to monitor, troubleshoot, and manage your infrastructure.

== Why Agentic AI for Admins?

Think of agentic AI as your copilot for cluster operations - ready to answer complex infrastructure questions using real-time telemetry and reasoning. 

Instead of navigating complex dashboards, YAML, and shell scripts, you'll try something new:

* “What’s consuming the most memory in our dev cluster right now?”

* “Were there any pod crashes in the last 30 minutes?”

* “Why did that Job fail this morning?”

These natural-language prompts get translated into real queries and results. The outcome? Faster issue resolution, increased system transparency, and lower barrier to operational insights for both technical and non-technical users.

That's an example of the power of agentic AI.

== Meet Your New Toolkit

In this lab, we'll use a preconfigured stack that includes:

* **https://github.com/meta-llama/llama-stack[Llama Stack]** - a flexible open-source framework developed by Meta to simplify the creation and deployment of advanced AI applications.
** Llama Stack provides a unified API layer for inference, RAG, agents, tools, safety, evals and telemetry.
** **Why It Matters for Enterprises**:
*** Codifies best practices across Gen AI tools
*** Enables reproducible, explainable, and extensible AI workflows
*** In short, it lets you focus on value creation, not toolchain assembly.

IMPORTANT: You could also use frameworks like LangChain or CrewAI instead of Llama Stack with OpenShift AI. All of these tools help you build agentic AI workflows with reasoning, tool use, and orchestration. Llama Stack is Red Hat's recommended, and supported, framework.

* **OpenShift MCP Server**  - allows our LLM agent to interact with our live OpenShift cluster. This MCP server is namespace-scoped. We will use it to interact with the OpenShift resources within the `lls-demo` namespace.

* **https://huggingface.co/meta-llama/Llama-Guard-3-1B[Llama Guard]** - optional input/output filtering for responsible AI interactions using another safety-trained LLM that is being served on our cluster.

== Deploy the Slack MCP Server

* In the terminal view, paste the following command to deploy the MCP server:

[source,console,role="execute"]
----
oc apply -k workshop_code/slack-mcp/ -n lls-demo
----

This will create the mcp server deployment and service.

image:code/successful_deploy.png[]

== Verify successful deployment

1. In the terminal, run:

[source,console,role="execute"]
----
oc get pods -n lls-demo
----

This will show all pods in the namespace within which we just deployed our slack mcp server. You should see our `slack-mcp-server` pod up and running, or in the process of starting:

image::code/mcp_status.png[width="75%"]

=== Review the Llama Stack ConfigMap

1. Go to the OpenShift console: 

+
https://console-openshift-console.{openshift_cluster_ingress_domain}
+

2. In the Administrator perspective, select API Explorer.

+
image:llama/api_explorer.png[width="75%"]
+

3. Search `ConfigMap` in the search bar and select the resource type.

4. Ensure you are in the right project. Type `lls-demo` in the project search bar.

+
image::llama/lls-project.png[width="75%"]
+

5. Select `Instances` and the available instance.

+
image:llama-configmap.png[width="75%"]
+

6. Select `YAML` to view the configmap contents.

The ConfigMap contains our crucial Llama Stack connections and is where we add in our various providers and models we want to use. Our models are already pre-configured (**Granite 8b** for chat and **Llama Guard** for safety).

=== Add Slack MCP Server to Llama Stack Configuration

// * Click on `Workloads` -> `ConfigMaps`

// image:llama/configmap-nav.png[width="75%"]

// * Find our `llama-stack-config`

// image::llama/lls_config.png[width="75%"]

// * Click on the `yaml` tab.

// image::llama/yaml-tab.png[width="75%"]

7. Add the following to the end of llama-stack-config `ConfigMap` in the `tool_groups` section:

[source,console,role=execute,subs=attributes+]
----
- toolgroup_id: mcp::slack
    provider_id: model-context-protocol
    mcp_endpoint:
    uri: "http://slack-mcp-server:80/sse"
----

image:llama/configmap_tool.png[width="75%"]

NOTE: Be mindful of YAML formatting. Ensure the indentation matches the example above. In our workshop this section is commented out, so you can also simply uncomment it.

8. Click `Save` to persist the changes.

=== Check your Llama Stack configuration with the Llama-Stack-Client

1. From your workbench you generated in the previous section, let's check the Llama Stack configuration once again via CLI:

+
[source,bash,options="wrap",role="execute"]
----
llama-stack-client --endpoint http://llamastack-service.lls-demo.svc.cluster.local:8321 providers list
----

[source,bash,options="wrap",role="execute"]
----
llama-stack-client --endpoint http://llamastack-service.lls-demo.svc.cluster.local:8321 models list
----

== View Your Deployment

1. Go back to the OpenShift console: 

+
https://console-openshift-console.{openshift_cluster_ingress_domain}
+

2. Now, select the `Developer` perspective.

+
image:llama/dev_perspective.png[width="75%"]
+

3. In case you are not in our specific project where the Llama Stack resources are deployed, search for the `lls-demo` namespace:

+
image:llama/find-namespace.png[width="75%"]
+

4. Select the `Topology` tab in the navigation bar as seen above.

In the Topology view, you will see four pods:

* **Llama Stack**: core server that connects Gen AI models to real-world tools and services. Our Llama Stack server handles the complex orchestration of turning natural language requests into real API calls, tool calls, and responses while maintaining context and security.
* **OCP MCP Server**: an MCP Server with tools to help our model interact with and understand OpenShift.
* **Slack MCP Server**: an MCP Server with tools to help our model interact with and understand Slack.
* **Llama Stack Playground**: A streamlit UI to interact with the system.

image::llama/see_topology.png[width="75%"]

Feel free to poke around and explore the deployment.

5. Select the Llama Stack playground hyperlink to open the UI.

image:llama/playground_link.png[width="75%"]

Now you will see the "playground" user interface. This application was created in the upstream project for the purposes of demonstration and experimentation and is **not** a supported component of our downstream OpenShift AI product.

image::llama/playground_ui.png[width="75%"]

== Configure the AI Agent

Within the application you'll find a familiar chat interface with some selection options on the left-hand side.

1. Select our model from the drop down

+
[.bordershadow]
image::llama/model_selection.png[width="75%"]
+

2. Set `Processing mode` -> `Agent-based`, giving us access to the tools we have configured via the MCP servers.

+
image::llama/agent_selection.png[width="75%"]
+

3. Enable the OpenShift MCP tool group.

+
image::llama/mcp_servers.png[width="75%"]
+

4. Once the MCP server is selected, you can peruse the active tools available.

+
image:llama/active_tools.png[width="75%"]
+

Everything else can remain unchanged.

== Investigate our OpenShift Resources

The active tools information will give you guidance into how to interact with the model in chat to activate the tool calls correctly.

NOTE: Our Llama Stack deployment is namespace-scoped. Therefore, in this activity, we will only be able to interact with the OpenShift resources within the `lls-demo` namespace containing the Llama Stack server and playground.

In the chat, enter:

[source,console,role=execute,subs=attributes+]
----
List all pods in the lls-demo namespace.
----

Response output will vary. But you will see it activate the tool, and give you a response. Something like this:

image::llama/ocp_response_example.png[width="75%"]

Let's try something else:

[source,console,role=execute,subs=attributes+]
----
Get logs for the <ocp-mcp-server-pod-name> pod in the lls-demo namespace.
----

IMPORTANT: You will need to replace the `<ocp-mcp-server-pod-name>` with the actual pod name. You can find the pod name from the response to the `list all pods` prompt.

You will again see that the associate tool is activated, and the model will then generate a response from the context provided by the tool call.

Feel free to experiment further with the tools available.

NOTE: We are using a small model, which is not optimal for agentic AI performance in production use cases. For demos and non-critical work, it can be quite impressive! However, some responses may be incomplete or inconsistent, and the model may hallucinate or misinterpret results if the tool output is vague or malformed or if we are asking it to engage with multiple MCP servers (like in this workshop!). The demonstration is meant to highlight the potential of natural language interfaces for interacting with infrastructure, and how emerging tools like Llama Stack and MCP can reduce the barrier to entry for understanding system behavior and save valuable time and effort.

=== Post a Message to our Slack Workspace

With our Slack MCP Server connected to Llama Stack, we can extend our agentic AI experience beyond Kubernetes and into team collaboration tools (among many other possibilities!).

This MCP server bridges your AI agent with a Slack workspace to fetch approved data.

**Why this matters:**

* SREs and DevOps teams often work across multiple collaboration channels.

* By giving your AI visibility into Slack, you can use natural language to check team communication spaces without switching tools.

==== Activate the Slack MCP Server

In the left-hand menu, select the `Slack MCP Server` tool group. This will clear the current chat. You may keep the OpenShift MCP Server enabled as well or deactivate it. 

image::llama/mcp_servers_2nd.png[width="75%"]

NOTE: If you experience hallucinations with both MCP servers enabled or after a few different chat interactions, you may need to refresh your browser to reset the chat.

==== In the Llama Stack Playground chat interface, type:

[source,console,role=execute,subs=attributes+]
----
List all Slack channels in our Slack workspace.
----

Now, let's post a message to our Slack workspace.

NOTE: Substitute `<insert-message-here>` with something funny but **mostly appropriate** that I can laugh at later. Anything you fancy!

[source,console,role=execute,subs=attributes+]
----
Post a message to the #etx-ai-singapore channel: "<insert-message-here>".
----

=== Send logs to Slack

Now, let's try out a very real use case for this! It may not be done through a chat UI like this, but it's a good example of how you can use agentic AI to help you with your work. In a production environment, you would likely use a more robust automation to send logs or other information from your OpenShift cluster to Slack.

Remember your role! You are a DevOps engineer. Let's send the logs for the ocp-mcp-server pod to the #etx-ai-singapore channel.

If you do not have both MCP servers enabled, make sure they are now.

If you no longer have the pod logs, let's retrieve them again:

[source,console,role=execute,subs=attributes+]
----
Get logs for the <ocp-mcp-server-pod-name> pod in the lls-demo namespace.
----

NOTE: You will need to replace the `<ocp-mcp-server-pod-name>` with the actual pod name. You can find the pod name from the response to the list all pods prompt.

Now, in a second message, post the logs to the #etx-ai-singapore channel:

[source,console,role=execute,subs=attributes+]
----
Post this log information to the #etx-ai-singapore slack workspace channel.
----

Expected output should look something like this:

image::llama/post_message.png[width="75%"]

Don't forget, if you do not get this result it's okay! It's because of our small model. Try refreshing the window and trying again.

=== Add Responsible AI Shields

Enterprise AI deployments require robust safety measures, especially when AI agents have access to critical infrastructure. **Guardian models** like Llama Guard serve as intelligent safety filters that evaluate both user inputs and AI outputs in real-time.

To enforce guardrails on inputs and outputs, select the **Llama Guard** model under the `Input Shields` and `Output Shields` form fields:

image::llama/guards.png[width="75%"]

Test the guards by asking the AI to perform an inappropriate action - you'll see how Llama Guard intercepts and blocks problematic requests!

== Summary: What You Did

In this module, you:

* Acted as a DevOps engineer using AI for cluster resource insight
* Integrated your own LLM with a tool-using agent.
* Explored OpenShift resources with natural language
* Interacted with a Slack workspace using natural language
* Added AI guardrails with input/output shields.

You just used AI to reduce operational complexity and speed up workflows! 