:imagesdir: ../assets/images
[#closing-summary]
= Closing Summary

Over the course of this hands-on workshop, you experienced how generative AI and agentic systems can be:

* Deployed and secured
* Exposed as scalable services
* Integrated into real workflows
* Observed and monitored for impact

All within an enterprise-ready **OpenShift AI** environment.

== What You Accomplished

You moved between multiple roles today:

* **Platform engineer** - deploying, exposing, and governing AI services through MaaS infrastructure.
* **AI Developer** - integrating LLMs into code workflows and building intelligent apps
* **DevOp engineer** - enhancing and improving operational insight with agentic tools like MCP and Llama Stack
* **Technical decision maker** - leveraging analytics to track usage, optimize cost, and plan for growth.

== Understanding Model Size and Production Architecture

Throughout this workshop, you've worked with smaller models (like our 8B parameter Granite model). While these models are impressive for demonstrations, it's important to understand their strengths and limitations in production environments.

**Why Small Models Excel:**

* **Specialized Tasks**: Small models perform exceptionally well when focused on singular, well-defined purposes
* **Resource Efficiency**: Lower compute requirements mean faster responses and lower operational costs
* **Deployment Flexibility**: Can be deployed closer to the edge or in resource-constrained environments
* **Focused Training**: Easier to fine-tune for specific use cases and organizational vocabularies

**Production Architecture: Multiple Specialized Models**

In real enterprise deployments, you typically deploy **multiple small models**, each optimized for specific functions:

* **Infrastructure Model**: Specialized for Kubernetes/OpenShift operations and troubleshooting
* **Communication Model**: Optimized for Slack interactions, notifications, and team coordination
* **Security Model**: Focused on threat detection, compliance checking, and audit responses
* **Code Model**: Specialized for code review, generation, and technical documentation

**Why Our Multi-Tool Demo Has Limitations:**

* **Context Switching**: Small models struggle to maintain context when switching between different tool domains
* **Knowledge Overlap**: Trying to be "good at everything" dilutes performance in specific areas
* **Hallucination Risk**: Models may confuse concepts when handling diverse, unrelated tasks simultaneously

**Production Best Practice**: Deploy purpose-built models that excel in their specific domains, then orchestrate them through a routing layer that directs queries to the most appropriate specialist model.

This approach delivers both the efficiency of small models and the comprehensive coverage needed for enterprise operations.

== What's Next?

**Explore More**

* https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/latest[OpenShift AI Documentation]
* https://developers.redhat.com/products/openshift-dev-spaces/overview[Try Red Hat OpenShift Dev Spaces]
* https://github.com/continuedev/continue[Continue GitHub Repository]
* https://llama-stack.readthedocs.io/en/latest/[Llama Stack Documentation]
* https://modelcontextprotocol.io/overview[Model Context Protocol (MCP)]